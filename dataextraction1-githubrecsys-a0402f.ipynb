{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport base64\nimport json\nimport requests\nimport os\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import PorterStemmer\nfrom scipy.sparse import vstack,csr_matrix,save_npz\n\n# Due to limit of GitHub api calls, only a subsample of repos will be explored at this time between the range (bg)th repo to (nd)th repo\nbg = 4500\nnd = 4600","metadata":{"execution":{"iopub.status.busy":"2021-12-22T01:42:36.204731Z","iopub.execute_input":"2021-12-22T01:42:36.205098Z","iopub.status.idle":"2021-12-22T01:42:37.842393Z","shell.execute_reply.started":"2021-12-22T01:42:36.204995Z","shell.execute_reply":"2021-12-22T01:42:37.841476Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"github_tokens = ['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']\nptr = 0\ngithub_token = github_tokens[ptr]\nMOD = 10000\nMOD_space = []\nfor i in range(MOD):\n    MOD_space.append(str(i))\nMOD_space = ' '.join(MOD_space)\n\nsource_extentions = ['py','ipynb','cpp','c','cfg','js','json','vue',\"xml\",\"java\",\"sh\",\"php\",\"rb\",\"ts\"]\ntext = []\nsource = []\n\ndef github_read_file(username, repository_name, file_path):\n    headers = {}\n    global github_token\n    if github_token:\n        headers['Authorization'] = f\"token {github_token}\"\n        \n    url = f'https://api.github.com/repos/{username}/{repository_name}/contents/{file_path}'\n    r = requests.get(url, headers=headers)\n    r.raise_for_status()\n    data = r.json()\n    file_content = data['content']\n    file_content_encoding = data.get('encoding')\n    if file_content_encoding == 'base64':\n        file_content = base64.b64decode(file_content).decode()\n    return file_content\n\n\ndef get_files(username,repository_name,file_path):\n    file_content = github_read_file(username, repository_name, file_path)\n    return file_content\n\n\ndef get_contents(username, repository_name):\n    headers = {}\n    global github_token\n    if github_token:\n        headers['Authorization'] = f\"token {github_token}\"\n        \n    url = f'https://api.github.com/repos/{username}/{repository_name}/contents'\n    r = requests.get(url, headers=headers)\n    r.raise_for_status()    \n    data = r.json()\n    return data\n\ndef recur(user,repo,r):\n    headers = {}\n    global github_token,text,source\n    if github_token:\n        headers['Authorization'] = f\"token {github_token}\"\n    for i in r.json():\n        if i[\"name\"].startswith('.'):\n            continue\n        if i['type'] == 'file':\n            if i['name'].split('.')[-1] == 'md' or i['name'].split('.')[-1] == 'txt':\n                text.append(i)\n            elif i['name'].split('.')[-1] in source_extentions :\n                source.append(i)\n        elif i['type'] == 'dir':\n            url = f'https://api.github.com/repos/{user}/{repo}/contents/' + i['path']\n            r = requests.get(url, headers=headers)\n            r.raise_for_status()\n            recur(user,repo,r)\n        \ndef get_filenames(user, repo):\n    headers = {}\n    global github_token,source,text\n    source = []\n    text = []\n    if github_token:\n        headers['Authorization'] = f\"token {github_token}\"\n    url = f'https://api.github.com/repos/{user}/{repo}/contents'\n    try:\n        r = requests.get(url, headers=headers)\n        r.raise_for_status()\n        recur(user,repo,r)\n    except:\n        print(\"error @ get_filenames\",user,repo)\n \ndef extract_words(user,repo,text):\n    puntuation = '1234567890-=!@#$%^&*()+[]{};:\"\\'|\\\\<>~/?`<>.,'\n    out = \"\"\n    ps = PorterStemmer()    \n    for i in text:\n        try:\n            data = get_files(user,repo,i['path'])\n            for i in puntuation:\n                data = data.replace(i,\" \")\n            data = data.replace(\"\\n\",' ')\n            data = re.sub(r'\\b\\w{1,2}\\b', '', data)\n            data += ' '\n            data = re.sub(\"\\s\\s+\" , \" \", data)\n            if len(data)>0 and data[0]==' ':\n                data = data[1:]\n            data = data.lower()\n            ps.stem(data)\n            data = ' '.join([str(hash(i)%MOD) for i in data.split(' ')])\n            out += data + ' '\n        except:\n            print(\"error reading\",i[\"name\"])\n            pass\n    return out","metadata":{"execution":{"iopub.status.busy":"2021-12-21T14:08:07.309567Z","iopub.execute_input":"2021-12-21T14:08:07.309859Z","iopub.status.idle":"2021-12-21T14:08:07.341115Z","shell.execute_reply.started":"2021-12-21T14:08:07.309808Z","shell.execute_reply":"2021-12-21T14:08:07.340096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code to import all significant repos of users from users.txt file\n# Possible errors if the github token is invalid\ndef get_repos(path):\n    with open(path,'r') as f:\n        save = [x.strip() for x in f.readlines()]\n    headers = {}\n    global github_token\n    if github_token:\n        headers['Authorization'] = f\"token {github_token}\"\n    l = []\n    ec = 0\n    Error_Limit = 30\n    for i in save:\n        try:\n            r = requests.get(\"https://api.github.com/users/{}/repos\".format(i),headers = headers)\n            r.raise_for_status()\n            for e in r.json():\n                if e['full_name'] not in l:\n                    l.append(e['full_name'])\n        except:\n            ec+=1\n            print(\"error @ \",i)\n        if ec>Error_Limit:\n            break\n    out = open(\"./repos.txt\",'w')\n    print(out.writelines([i+'\\n' for i in l]))\n    out.close()\n#get_repos(\"../input/githubrecsys1/users.txt\")","metadata":{"execution":{"iopub.status.busy":"2021-12-21T14:08:07.342466Z","iopub.execute_input":"2021-12-21T14:08:07.342721Z","iopub.status.idle":"2021-12-21T14:08:07.358495Z","shell.execute_reply.started":"2021-12-21T14:08:07.342692Z","shell.execute_reply":"2021-12-21T14:08:07.357621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/githubrecsys1/repos.txt\",\"r\") as f:\n    out = [l.strip() for l in f]\nind = {}\nfor i in range(len(out)):\n    ind[out[i]] = i\n    \nRepo_text_hashed = []\nRepo_source_hashed = []\nfor i in out[bg:nd]:\n    print(i)\n    ptr+=1\n    global github_token\n    ptr %= len(github_tokens)\n    github_token = github_tokens[ptr]\n    get_filenames(*i.split(\"/\"))\n    Repo_text_hashed.append(extract_words(*i.split(\"/\"),text))\n    Repo_source_hashed.append(extract_words(*i.split(\"/\"),source))\n    \nvec1 = TfidfVectorizer()\nvec2 = TfidfVectorizer()\nvec1.fit(Repo_text_hashed)\nvec2.fit(Repo_source_hashed)\nRepo_text_points = []\nRepo_source_points = []\n\ndef get_mapper(voc):\n    def mapper(key):\n        if key=='' or int(key) >= MOD:\n            key = 0\n        if key not in voc:\n            return len(voc)\n        return voc[key]\n    return mapper\n\nmapper = get_mapper(vec1.vocabulary_)\nfor i in Repo_text_hashed:\n    res = map(lambda x:mapper(x),i.split(' '))\n    temp = np.bincount(list(res),minlength = MOD)\n    if len(i.split(' '))>0:\n        temp = temp / np.array([len(i.split(' '))],dtype = np.float64)\n    Repo_text_points.append(csr_matrix(temp , dtype  = np.float64))\n    \nmapper = get_mapper(vec2.vocabulary_)\nfor i in Repo_source_hashed:\n    res = map(lambda x:mapper(x),i.split(' '))\n    temp = np.bincount(list(res),minlength = MOD)\n    if len(i.split(' '))>0:\n        temp = temp / np.array([len(i.split(' '))],dtype = np.float64)\n    Repo_source_points.append(csr_matrix(temp,dtype = np.float64))\n\ndel Repo_source_hashed,Repo_text_hashed\nRepo_text_points = vstack(Repo_text_points)\nRepo_source_points = vstack(Repo_source_points)\n\nvec1idf = vstack([csr_matrix(np.concatenate([vec1.idf_,np.zeros(MOD - len(vec1.idf_))],axis=0))])\nvec2idf = vstack([csr_matrix(np.concatenate([vec2.idf_,np.zeros(MOD - len(vec2.idf_))],axis=0))])\n\nRepo_text_points = vec1idf.multiply(Repo_text_points)\nRepo_source_points = vec2idf.multiply(Repo_source_points)\n\n#Repo_similarity_text = cosine_similarity(Repo_text_points,Repo_text_points)\n#Repo_similarity_source = cosine_similarity(Repo_source_points,Repo_source_points)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T14:08:07.360642Z","iopub.execute_input":"2021-12-21T14:08:07.361144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_npz(\"./Repo_TextFeature_{}:{}\".format(bg,nd),Repo_text_points)\nsave_npz(\"./Repo_SourceFeature_{}:{}\".format(bg,nd),Repo_source_points)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}